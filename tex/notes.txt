- Process iteration detailed in `git log`
- Using a CNN with the toy game and a deterministic level (simple, learn to always move right) is easily trained given a sufficiently long period of time, but with a more complicated level, our results are not great in even an order of magnitude more episodes.
- Using the CNN with the same setup but a random level each time gives terrible results, with gradients and expected q-values going to 0 for any reasonably small number of episodes.
- Switching to a state vector representation (stacking state vects to make a matrix for the history), bypassing the conv layer in order to further explore performance in a reasonable training period, we see that a deterministic level (either simple or complicated, involving a wait then move) is easily trained in ~4k episodes (only 10 minutes!). This is a promising sign that the algorithm is implemented correctly.
- We try experimenting here and see that switching gamma from 0.90 to 0.99 (future rewards worth more, so we are more risk averse and more willing to just wait) leads to a much worse result, as the model often learns a local maximum of just staying still. Also switching to a larger history size than 2 only increases training time with not much improvement to performance.
- Using a history of 3 (which turns out to be the best in this case), we make the game non-deterministic and randomize the starting position of the player, keeping everything else the same. In this case, even after 100k episodes, our results are not good, with it either acting on par with random, or having all q-estimates go to 0 and it will stay still forever (always STAY, since this never results in death). Starting to train again (higher epsilon, more randomness) increases the loss but at this point the gradients have been wiped out and not much changes.
- Switching to a smaller history again, and this time concatenating state vectors. Also add a 2nd FC layer since no conv layers. After ~200k episodes, it wins about of the randomly-selected games we try. After ~800k episodes, it wins many, but not all. This was finally successful!
- Now let's try encoding less into the reward function, by removing the distance to the end. That was less successful. After ~800k episodes, our performance is terrible, with the model always moving right or getting stuck. Time to revert! (FC2-H2-NR-BAD)
- Time to try with images again, this time with a history size of 2. Will train for way more than ~100k episodes, which is what we tried last time. Didn't give us very good results even after ~600k episodes, also noticed that the loss never changes.
- Try to clip the gradient deltas at 1 and -1, also didn't give good results.
- Now unclipped, still just learns to always go right after millions of episodes.
- Back to state vectors in our toy game, but now 2D. After training for 900k episodes, we win about 90% of the games.
- 10% on random, 44% on greedy for TG2D-H
- FC 128 x2 gives peak of 55% on TG2D-H, average around 33%
- try with different architechture...
